
@article{neal_markov_2000,
	title = {Markov {Chain} {Sampling} {Methods} for {Dirichlet} {Process} {Mixture} {Models}},
	volume = {9},
	issn = {1061-8600, 1537-2715},
	url = {http://www.tandfonline.com/doi/abs/10.1080/10618600.2000.10474879},
	doi = {10.1080/10618600.2000.10474879},
	language = {en},
	number = {2},
	urldate = {2016-07-11},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Neal, Radford M.},
	month = jun,
	year = {2000},
	pages = {249--265}
}

@article{escobar_bayesian_1995,
	title = {Bayesian {Density} {Estimation} and {Inference} {Using} {Mixtures}},
	volume = {90},
	issn = {0162-1459},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1995.10476550},
	doi = {10.1080/01621459.1995.10476550},
	number = {430},
	urldate = {2016-07-11},
	journal = {Journal of the American Statistical Association},
	author = {Escobar, Michael D. and West, Mike},
	month = jun,
	year = {1995},
	pages = {577--588},
	file = {Snapshot:/Users/deanmarkwick/Zotero/storage/3EMU6ZEV/01621459.1995.html:text/html}
}

@article{maceachern_estimating_1998,
	title = {Estimating {Mixture} of {Dirichlet} {Process} {Models}},
	volume = {7},
	issn = {1061-8600, 1537-2715},
	url = {http://www.tandfonline.com/doi/abs/10.1080/10618600.1998.10474772},
	doi = {10.1080/10618600.1998.10474772},
	language = {en},
	number = {2},
	urldate = {2016-07-11},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Maceachern, Steven N. and Müller, Peter},
	month = jun,
	year = {1998},
	pages = {223--238}
}

@book{coles_introduction_2001,
	address = {London},
	series = {Springer {Series} in {Statistics}},
	title = {An {Introduction} to {Statistical} {Modeling} of {Extreme} {Values}},
	isbn = {978-1-84996-874-4 978-1-4471-3675-0},
	url = {http://link.springer.com/10.1007/978-1-4471-3675-0},
	urldate = {2016-07-12},
	publisher = {Springer London},
	author = {Coles, Stuart},
	year = {2001}
}

@book{west_hyperparameter_1992,
	title = {Hyperparameter estimation in {Dirichlet} process mixture models},
	publisher = {Duke University ISDS Discussion Paper{\textbackslash}\# 92-A03},
	author = {West, Mike},
	year = {1992}
}

@article{carpenter_bob_stan:_2016,
	title = {Stan: {A} probabilistic programming language},
	journal = {J Stat Softw},
	author = {{Carpenter, Bob} and {Gelman, Andrew} and {Hoffman, Matt} and {Lee, Daniel} and {Goodrich, Ben} and {Betancourt, Michael} and {Brubaker, Michael A} and {Guo, Jiqiang} and {Li, Peter} and {Riddell, Allen}},
	year = {2016}
}

@article{ferguson_bayesian_1973,
	title = {A {Bayesian} {Analysis} of {Some} {Nonparametric} {Problems}},
	volume = {1},
	issn = {0090-5364},
	url = {http://www.jstor.org/stable/2958008},
	abstract = {The Bayesian approach to statistical problems, though fruitful in many ways, has been rather unsuccessful in treating nonparametric problems. This is due primarily to the difficulty in finding workable prior distributions on the parameter space, which in nonparametric ploblems is taken to be a set of probability distributions on a given sample space. There are two desirable properties of a prior distribution for nonparametric problems. (I) The support of the prior distribution should be large--with respect to some suitable topology on the space of probability distributions on the sample space. (II) Posterior distributions given a sample of observations from the true probability distribution should be manageable analytically. These properties are antagonistic in the sense that one may be obtained at the expense of the other. This paper presents a class of prior distributions, called Dirichlet process priors, broad in the sense of (I), for which (II) is realized, and for which treatment of many nonparametric statistical problems may be carried out, yielding results that are comparable to the classical theory. In Section 2, we review the properties of the Dirichlet distribution needed for the description of the Dirichlet process given in Section 3. Briefly, this process may be described as follows. Let X be a space and A a σ-field of subsets, and let α be a finite non-null measure on (X, A). Then a stochastic process P indexed by elements A of A, is said to be a Dirichlet process on (X, A) with parameter α if for any measurable partition (A1, ⋯, Ak) of X, the random vector (P(A1), ⋯, P(Ak)) has a Dirichlet distribution with parameter (α(A1), ⋯, α(Ak)). P may be considered a random probability measure on (X, A), The main theorem states that if P is a Dirichlet process on (X, A) with parameter α, and if X1, ⋯, Xn is a sample from P, then the posterior distribution of P given X1, ⋯, Xn is also a Dirichlet process on (X, A) with a parameter α + ∑n 1 δxi , where δx denotes the measure giving mass one to the point x. In Section 4, an alternative definition of the Dirichlet process is given. This definition exhibits a version of the Dirichlet process that gives probability one to the set of discrete probability measures on (X, A). This is in contrast to Dubins and Freedman [2], whose methods for choosing a distribution function on the interval [0, 1] lead with probability one to singular continuous distributions. Methods of choosing a distribution function on [0, 1] that with probability one is absolutely continuous have been described by Kraft [7]. The general method of choosing a distribution function on [0, 1], described in Section 2 of Kraft and van Eeden [10], can of course be used to define the Dirichlet process on [0, 1]. Special mention must be made of the papers of Freedman and Fabius. Freedman [5] defines a notion of tailfree for a distribution on the set of all probability measures on a countable space X. For a tailfree prior, posterior distribution given a sample from the true probability measure may be fairly easily computed. Fabius [3] extends the notion of tailfree to the case where X is the unit interval [0, 1], but it is clear his extension may be made to cover quite general X. With such an extension, the Dirichlet process would be a special case of a tailfree distribution for which the posterior distribution has a particularly simple form. There are disadvantages to the fact that P chosen by a Dirichlet process is discrete with probability one. These appear mainly because in sampling from a P chosen by a Dirichlet process, we expect eventually to see one observation exactly equal to another. For example, consider the goodness-of-fit problem of testing the hypothesis H0 that a distribution on the interval [0, 1] is uniform. If on the alternative hypothesis we place a Dirichlet process prior with parameter α itself a uniform measure on [0, 1], and if we are given a sample of size n ≥ 2, the only nontrivial nonrandomized Bayes rule is to reject H0 if and only if two or more of the observations are exactly equal. This is really a test of the hypothesis that a distribution is continuous against the hypothesis that it is discrete. Thus, there is still a need for a prior that chooses a continuous distribution with probability one and yet satisfies properties (I) and (II). Some applications in which the possible doubling up of the values of the observations plays no essential role are presented in Section 5. These include the estimation of a distribution function, of a mean, of quantiles, of a variance and of a covariance. A two-sample problem is considered in which the Mann-Whitney statistic, equivalent to the rank-sum statistic, appears naturally. A decision theoretic upper tolerance limit for a quantile is also treated. Finally, a hypothesis testing problem concerning a quantile is shown to yield the sign test. In each of these problems, useful ways of combining prior information with the statistical observations appear. Other applications exist. In his Ph. D. dissertation [1], Charles Antoniak finds a need to consider mixtures of Dirichlet processes. He treats several problems, including the estimation of a mixing distribution, bio-assay, empirical Bayes problems, and discrimination problems.},
	number = {2},
	urldate = {2016-08-04},
	journal = {The Annals of Statistics},
	author = {Ferguson, Thomas S.},
	year = {1973},
	pages = {209--230}
}

@article{antoniak_mixtures_1974,
	title = {Mixtures of {Dirichlet} {Processes} with {Applications} to {Bayesian} {Nonparametric} {Problems}},
	volume = {2},
	issn = {0090-5364},
	url = {http://www.jstor.org/stable/2958336},
	abstract = {A random process called the Dirichlet process whose sample functions are almost surely probability measures has been proposed by Ferguson as an approach to analyzing nonparametric problems from a Bayesian viewpoint. An important result obtained by Ferguson in this approach is that if observations are made on a random variable whose distribution is a random sample function of a Dirichlet process, then the conditional distribution of the random measure can be easily calculated, and is again a Dirichlet process. This paper extends Ferguson's result to cases where the random measure is a mixing distribution for a parameter which determines the distribution from which observations are made. The conditional distribution of the random measure, given the observations, is no longer that of a simple Dirichlet process, but can be described as being a mixture of Dirichlet processes. This paper gives a formal definition for these mixtures and develops several theorems about their properties, the most important of which is a closure property for such mixtures. Formulas for computing the conditional distribution are derived and applications to problems in bio-assay, discrimination, regression, and mixing distributions are given.},
	number = {6},
	urldate = {2017-02-08},
	journal = {The Annals of Statistics},
	author = {Antoniak, Charles E.},
	year = {1974},
	pages = {1152--1174}
}

@article{kottas_nonparametric_2006,
	title = {Nonparametric {Bayesian} survival analysis using mixtures of {Weibull} distributions},
	volume = {136},
	issn = {0378-3758},
	url = {http://www.sciencedirect.com/science/article/pii/S0378375804003465},
	doi = {10.1016/j.jspi.2004.08.009},
	abstract = {Bayesian nonparametric methods have been applied to survival analysis problems since the emergence of the area of Bayesian nonparametrics. However, the use of the flexible class of Dirichlet process mixture models has been rather limited in this context. This is, arguably, to a large extent, due to the standard way of fitting such models that precludes full posterior inference for many functionals of interest in survival analysis applications. To overcome this difficulty, we provide a computational approach to obtain the posterior distribution of general functionals of a Dirichlet process mixture. We model the survival distribution employing a flexible Dirichlet process mixture, with a Weibull kernel, that yields rich inference for several important functionals. In the process, a method for hazard function estimation emerges. Methods for simulation-based model fitting, in the presence of censoring, and for prior specification are provided. We illustrate the modeling approach with simulated and real data.},
	number = {3},
	urldate = {2017-03-21},
	journal = {Journal of Statistical Planning and Inference},
	author = {Kottas, Athanasios},
	month = mar,
	year = {2006},
	keywords = {Censored observations, Dirichlet process mixture models, Hazard function, Survival function},
	pages = {578--596}
}

@inproceedings{kottas_dirichlet_2006,
	title = {Dirichlet process mixtures of beta distributions, with applications to density and intensity estimation},
	booktitle = {Workshop on {Learning} with {Nonparametric} {Bayesian} {Methods}, 23rd {International} {Conference} on {Machine} {Learning} ({ICML})},
	author = {Kottas, Athanasios},
	year = {2006}
}

@book{gelman_bayesian_2014,
	title = {Bayesian {Data} {Analysis}},
	volume = {2},
	publisher = {Chapman \& Hall/CRC Boca Raton, FL, USA},
	author = {Gelman, Andrew and Carlin, John B and Stern, Hal S and Rubin, Donald B},
	year = {2014}
}

@article{sethuraman_constructive_1994,
	title = {A constructive definition of {Dirichlet} priors},
	journal = {Statistica sinica},
	author = {Sethuraman, Jayaram},
	year = {1994},
	pages = {639--650}
}

@article{gelman_efficient_1996,
	title = {Efficient {Metropolis} jumping rules},
	author = {Gelman, Andrew and Roberts, Gareth O and Gilks, Walter R and {others}},
	year = {1996}
}

@book{lawless_statistical_2011,
	title = {Statistical models and methods for lifetime data},
	volume = {362},
	publisher = {John Wiley \& Sons},
	author = {Lawless, Jerald F},
	year = {2011}
}

@book{wickham_advanced_2014,
	title = {Advanced r},
	publisher = {CRC Press},
	author = {Wickham, Hadley},
	year = {2014}
}

@article{hastings_monte_1970,
	title = {Monte {Carlo} {Sampling} {Methods} {Using} {Markov} {Chains} and {Their} {Applications}},
	volume = {57},
	issn = {0006-3444},
	url = {http://www.jstor.org/stable/2334940},
	doi = {10.2307/2334940},
	abstract = {A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.},
	number = {1},
	urldate = {2017-07-28},
	journal = {Biometrika},
	author = {Hastings, W. K.},
	year = {1970},
	pages = {97--109}
}

@article{geman_stochastic_1984,
	title = {Stochastic {Relaxation}, {Gibbs} {Distributions}, and the {Bayesian} {Restoration} of {Images}},
	volume = {PAMI-6},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.1984.4767596},
	abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (“annealing”), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel “relaxation” algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
	number = {6},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Geman, S. and Geman, D.},
	month = nov,
	year = {1984},
	keywords = {Additive noise, Annealing, Bayesian methods, Deformable models, Degradation, Energy states, Image restoration, Markov random fields, Stochastic processes, Temperature distribution, Gibbs distribution, MAP estimate, Markov random field, line process, relaxation, scene modeling, spatial degradation},
	pages = {721--741},
	file = {IEEE Xplore Abstract Record:/Users/deanmarkwick/Zotero/storage/MQCCW32S/4767596.html:text/html}
}

@inproceedings{teh_sharing_2005,
	title = {Sharing clusters among related groups: {Hierarchical} {Dirichlet} processes},
	booktitle = {Advances in neural information processing systems},
	author = {Teh, Yee W and Jordan, Michael I and Beal, Matthew J and Blei, David M},
	year = {2005},
	pages = {1385--1392}
}

@article{kim_variable_2006,
	title = {Variable selection in clustering via {Dirichlet} process mixture models},
	volume = {93},
	number = {4},
	journal = {Biometrika},
	author = {Kim, Sinae and Tadesse, Mahlet G and Vannucci, Marina},
	year = {2006},
	pages = {877--893}
}

@article{taddy_mixture_2012,
	title = {Mixture {Modeling} for {Marked} {Poisson} {Processes}},
	volume = {7},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/euclid.ba/1339878891},
	doi = {10.1214/12-BA711},
	abstract = {We propose a general inference framework for marked Poisson processes observed over time or space. Our modeling approach exploits the connection of nonhomogeneous Poisson process intensity with a density function. Nonparametric Dirichlet process mixtures for this density, combined with nonparametric or semiparametric modeling for the mark distribution, yield flexible prior models for the marked Poisson process. In particular, we focus on fully nonparametric model formulations that build the mark density and intensity function from a joint nonparametric mixture, and provide guidelines for straightforward application of these techniques. A key feature of such models is that they can yield flexible inference about the conditional distribution for multivariate marks without requiring specification of a complicated dependence scheme. We address issues relating to choice of the Dirichlet process mixture kernels, and develop methods for prior specification and posterior simulation for full inference about functionals of the marked Poisson process. Moreover, we discuss a method for model checking that can be used to assess and compare goodness of fit of different model specifications under the proposed framework. The methodology is illustrated with simulated and real data sets.},
	language = {EN},
	number = {2},
	urldate = {2018-10-10},
	journal = {Bayesian Analysis},
	author = {Taddy, Matthew A. and Kottas, Athanasios},
	month = jun,
	year = {2012},
	mrnumber = {MR2934954},
	zmnumber = {1330.62200},
	keywords = {Bayesian nonparametrics, Beta mixtures, Dirichlet process, Marked point process, Multivariate normal mixtures, Non-homogeneous Poisson process, Nonparametric regression},
	pages = {335--362},
	file = {Full Text PDF:/Users/deanmarkwick/Zotero/storage/BEQZBLI3/Taddy and Kottas - 2012 - Mixture Modeling for Marked Poisson Processes.pdf:application/pdf;Snapshot:/Users/deanmarkwick/Zotero/storage/QAZT9VIS/1339878891.html:text/html}
}

@article{tran_edward:_2016,
	title = {Edward: {A} library for probabilistic modeling, inference, and criticism},
	shorttitle = {Edward},
	url = {http://arxiv.org/abs/1610.09787},
	abstract = {Probabilistic modeling is a powerful approach for analyzing empirical information. We describe Edward, a library for probabilistic modeling. Edward's design reflects an iterative process pioneered by George Box: build a model of a phenomenon, make inferences about the model given data, and criticize the model's fit to the data. Edward supports a broad class of probabilistic models, efficient algorithms for inference, and many techniques for model criticism. The library builds on top of TensorFlow to support distributed training and hardware such as GPUs. Edward enables the development of complex probabilistic models and their algorithms at a massive scale.},
	urldate = {2018-10-10},
	journal = {arXiv:1610.09787 [cs, stat]},
	author = {Tran, Dustin and Kucukelbir, Alp and Dieng, Adji B. and Rudolph, Maja and Liang, Dawen and Blei, David M.},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.09787},
	keywords = {Statistics - Applications, Statistics - Computation, Computer Science - Artificial Intelligence, Computer Science - Programming Languages, Statistics - Machine Learning},
	file = {arXiv\:1610.09787 PDF:/Users/deanmarkwick/Zotero/storage/F7MCBW3Y/Tran et al. - 2016 - Edward A library for probabilistic modeling, infe.pdf:application/pdf;arXiv.org Snapshot:/Users/deanmarkwick/Zotero/storage/2IMLCEJZ/1610.html:text/html}
}

@article{salvatier_probabilistic_2016,
	title = {Probabilistic programming in {Python} using {PyMC}3},
	volume = {2},
	issn = {2376-5992},
	url = {https://peerj.com/articles/cs-55},
	doi = {10.7717/peerj-cs.55},
	abstract = {Probabilistic programming allows for automatic Bayesian inference on user-defined probabilistic models. Recent advances in Markov chain Monte Carlo (MCMC) sampling allow inference on increasingly complex models. This class of MCMC, known as Hamiltonian Monte Carlo, requires gradient information which is often not readily available. PyMC3 is a new open source probabilistic programming framework written in Python that uses Theano to compute gradients via automatic differentiation as well as compile probabilistic programs on-the-fly to C for increased speed. Contrary to other probabilistic programming languages, PyMC3 allows model specification directly in Python code. The lack of a domain specific language allows for great flexibility and direct interaction with the model. This paper is a tutorial-style introduction to this software package.},
	language = {en},
	urldate = {2018-10-10},
	journal = {PeerJ Computer Science},
	author = {Salvatier, John and Wiecki, Thomas V. and Fonnesbeck, Christopher},
	month = apr,
	year = {2016},
	pages = {e55},
	file = {Full Text PDF:/Users/deanmarkwick/Zotero/storage/RLCADIVV/Salvatier et al. - 2016 - Probabilistic programming in Python using PyMC3.pdf:application/pdf;Snapshot:/Users/deanmarkwick/Zotero/storage/6VDBWWCG/cs-55.html:text/html}
}